<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.2" />






<meta name="description" content="email：summer15y@163.com">
<meta property="og:type" content="website">
<meta property="og:title" content="HotSummer">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="HotSummer">
<meta property="og:description" content="email：summer15y@163.com">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="HotSummer">
<meta name="twitter:description" content="email：summer15y@163.com">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    }
  };
</script>





  <title> HotSummer </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="//schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="//schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">HotSummer</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="//schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/01/19/ML-F-PCA/" itemprop="url">
                  主成分分析
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-01-19T17:07:22+08:00" content="2017-01-19">
              2017-01-19
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index">
                    <span itemprop="name">ML</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>主成分分析（Principal Components Analysis，简称PCA）是一种数据降维方法，在降低数据维度的同时，尽可能保留原始数据的特征。PCA的原理解释有多个版本，这篇笔记只讨论其中一个。</p>
<h3 id="参考">参考</h3>
<ul>
<li><a href="http://www.qiujiawei.com/linear-algebra-17/" target="_blank" rel="noopener">线性代数之主成分分析(PCA)算法</a></li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/%E7%99%BD%E5%8C%96" target="_blank" rel="noopener">白化</a></li>
<li><a href="https://my.oschina.net/findbill/blog/543485" target="_blank" rel="noopener">白化（Whitening）：PCA vs. ZCA</a></li>
</ul>
<h3 id="原理">原理</h3>
<p>PCA最终目的是对表示 <span class="math inline">\(m\)</span> 条 <span class="math inline">\(n\)</span> 维（n个特征）数据的矩阵 <span class="math inline">\(X_{m \times n}\)</span> 做线性变换，得到矩阵 <span class="math inline">\(Y_{m \times n}\)</span>，再从中选取 k 维：</p>
<p><span class="math display">\[ Y_{m \times n} = X_{m \times n} P_{n \times n} \]</span></p>
<p><strong>怎样求解变换矩阵 <span class="math inline">\(P\)</span>？</strong> 我们想要的理想效果是 <span class="math inline">\(Y\)</span> 的 <span class="math inline">\(n\)</span> 个维度相互独立，这样每个维度保留各自的特征互不干扰，然后选取 <span class="math inline">\(n\)</span> 个维度中方差最大的 k 个维度，包含最丰富的信息。</p>
<p>先介绍矩阵 <span class="math inline">\(X\)</span> 协方差矩阵的计算公式，这里假设已经处理 <span class="math inline">\(X\)</span> 每个维度使之均值为0：</p>
<p><span class="math display">\[ C_X = \frac{1}{n}\sum_{i=1}^{m}(X[:,i]^T X[:,i]) = \frac{1}{n} X^TX \]</span></p>
<p>其中 <span class="math inline">\(X[:,i]\)</span> 表示取 <span class="math inline">\(X\)</span> 的第 <span class="math inline">\(i\)</span> 列特征。这里引入 <a href="/2016/09/02/Math-PrStats-cheatSheet/" title="协方差">协方差</a> 的概念，如果两个随机变量相互独立，那么它们的协方差为0。协方差矩阵的每个元素 <span class="math inline">\(c_{ij}\)</span> 等于对应 <span class="math inline">\(i,j\)</span> 维度的协方差，而对角元素等于对应维度的方差，所以我们的目标是 <strong>使 <span class="math inline">\(Y\)</span> 的协方差矩阵为对角矩阵</strong>。</p>
<p>同样，有：</p>
<p><span class="math display">\[ C_Y = \frac{1}{n} Y^T Y = \frac{1}{n} (XP)^T(XP) = P^T (\frac{1}{n}X^TX) P \\
= P^T C_X P \]</span></p>
<p>因为 <span class="math inline">\(C_X\)</span> 为实对称矩阵，故能够进行对角化：</p>
<p><span class="math display">\[ C_X = S\Lambda S^{-1} = S\Lambda S^{T}\]</span></p>
<p>其中 <span class="math inline">\(\Lambda\)</span> 为对角矩阵，其对角线元素为 <span class="math inline">\(C_X\)</span> 的特征值，而 <span class="math inline">\(S\)</span> 是正交矩阵，有 <span class="math inline">\(S^TS=I\)</span>。容易想到，如果令 <span class="math inline">\(P=S\)</span>：</p>
<p><span class="math display">\[ C_Y = (P^TS) \Lambda (P^TS)^T = \Lambda \]</span></p>
<p>这就成功将 <span class="math inline">\(C_Y\)</span> 变换为对角矩阵，所以顺利解得 <span class="math inline">\(P=S\)</span>。</p>
<p>最后一步选取 <span class="math inline">\(n\)</span> 维中方差最大的 <span class="math inline">\(k\)</span> 维以达到降维的最终目的。<span class="math inline">\(C_Y\)</span> 的对角元素对应 <span class="math inline">\(Y\)</span> 每个维度的方差，所以我们只需保留 <span class="math inline">\(k\)</span> 个最大对角元对应的 <span class="math inline">\(P\)</span> 的 <span class="math inline">\(k\)</span> 个列向量，同时也是 <span class="math inline">\(C_X\)</span> 的 <span class="math inline">\(k\)</span> 个最大特征值对应的特征向量，得到 <span class="math inline">\(n \times k\)</span> 的变换矩阵 <span class="math inline">\(P\)</span>，最后做线性变换 <span class="math inline">\(Y=XP\)</span>。</p>
<h3 id="实战">实战</h3>
<p><span class="math inline">\(m\)</span> 条 <span class="math inline">\(n\)</span> 维数据由矩阵 <span class="math inline">\(X_{m \times n}\)</span> 表示。以下是处理步骤：</p>
<ol type="1">
<li>对 <span class="math inline">\(X\)</span> 每个维度做中心化（可进一步做标准化）</li>
<li>求协方差矩阵 <span class="math inline">\(C_X = X^TX\)</span></li>
<li>对角化： <span class="math inline">\(C_X = S\Lambda S^{T}\)</span></li>
<li>从 <span class="math inline">\(S\)</span> 中挑选 <span class="math inline">\(k\)</span> 列特征值（<span class="math inline">\(\Lambda\)</span> 对角元）最大的特征向量得到 <span class="math inline">\(P\)</span></li>
<li><span class="math inline">\(Y_{m\times k}=X_{m\times n}P_{n\times k}\)</span></li>
</ol>
<p>调用 sklearn API： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data  <span class="comment"># 加载实验数据，有 4 维</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> decomposition</span><br><span class="line"><span class="comment"># 调用只需这一句，n_components 指定降维后的维度数 3</span></span><br><span class="line">pX = decomposition.PCA(n_components=<span class="number">3</span>).fit_transform(X)</span><br></pre></td></tr></table></figure></p>
<p>python 实现： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data <span class="comment"># 加载实验数据，有 4 维</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> linalg <span class="keyword">as</span> LA</span><br><span class="line"></span><br><span class="line">n_components = <span class="number">3</span> <span class="comment"># 指定降维后的维度数 3</span></span><br><span class="line"></span><br><span class="line">Xm = X - X.mean(axis=<span class="number">0</span>) <span class="comment"># 中心化</span></span><br><span class="line">Cx = Xm.T.dot(Xm) <span class="comment"># 协方差矩阵</span></span><br><span class="line">ev, S = LA.eigh(Cx) <span class="comment"># 对角化，S 特征向量按特征值升序排列</span></span><br><span class="line">P = S[:, [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>-n_components, <span class="number">4</span>)]] <span class="comment"># 选取特征向量，构造变换矩阵</span></span><br><span class="line">pX = Xm.dot(P)  <span class="comment"># 线性变换</span></span><br></pre></td></tr></table></figure></p>
<h3 id="白化">白化</h3>
<p>白化的目的：</p>
<ul>
<li>特征之间相关性较低</li>
<li>所有特征具有相同的方差</li>
</ul>
<p>前者可以用 <strong>不降维</strong> 的PCA做到。假设已经通过PCA得到变换矩阵 <span class="math inline">\(P_{n\times n}\)</span>，则 <span class="math inline">\(Y_{m \times n}=X_{m \times n}P_{n \times n}\)</span>。接下来需要达到后一个目标，使 <span class="math inline">\(C_Y=I\)</span>。</p>
<h4 id="pca白化">PCA白化</h4>
<p>由于 <span class="math inline">\(C_Y = \frac{1}{n} Y^T Y= \Lambda\)</span>，所以只需更新 <span class="math inline">\(Y_{PCAwhite}=Y\Lambda^{-1/2}\)</span></p>
<h4 id="zca白化">ZCA白化</h4>
<p>ZCA (Zero-phase Component Analysis Whitening) 在PCA白化的基础上，把数据变换回原空间。更新公式 <span class="math inline">\(Y_{ZCAwhite}=Y_{PCAwhite}P^T\)</span>，可检验 <span class="math inline">\(C_Y\)</span>，仍满足单位矩阵。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="//schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/01/18/ML-NN-backpropagation/" itemprop="url">
                  神经网络之反向传播与自编码器
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-01-18T14:14:56+08:00" content="2017-01-18">
              2017-01-18
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index">
                    <span itemprop="name">ML</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文主要对神经网络反向传播（Backpropagation Algorithm）的求导关键部分做笔记，并简要介绍自编码器概念（Autoencoder）。</p>
<h3 id="参考">参考</h3>
<ul>
<li><a href="http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial" target="_blank" rel="noopener">UFLDL</a></li>
<li><a href="http://www.csie.ntu.edu.tw/~htlin/" target="_blank" rel="noopener">林轩田老师</a> 教授的《机器学习技法》 12课：神经网络</li>
</ul>
<h3 id="符号">符号</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">符号</th>
<th style="text-align: center;">含义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\((x^i, y^i)\)</span></td>
<td style="text-align: center;">第 <span class="math inline">\(i\)</span> 个训练样本</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(h_{W,b}(x)\)</span></td>
<td style="text-align: center;">输入为 <span class="math inline">\(x\)</span> 时的假设输出，其中包含参数 <span class="math inline">\(W,b\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(a_i^l\)</span></td>
<td style="text-align: center;">第 <span class="math inline">\(l\)</span> 层 <span class="math inline">\(i\)</span> 单元的激活值，对第一层有 <span class="math inline">\(a_i^1=x_i\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(W^l_{ij}\)</span></td>
<td style="text-align: center;">连接第 <span class="math inline">\(l\)</span> 层 <span class="math inline">\(j\)</span> 单元和第 <span class="math inline">\(l+1\)</span> 层 <span class="math inline">\(i\)</span> 单元的参数</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(b^l_i\)</span></td>
<td style="text-align: center;">连接第 <span class="math inline">\(l\)</span> 层偏置单元和第 <span class="math inline">\(l+1\)</span> 层 <span class="math inline">\(i\)</span> 单元的参数</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(z_i^l\)</span></td>
<td style="text-align: center;">第 <span class="math inline">\(l\)</span> 层 <span class="math inline">\(i\)</span> 单元所有输入的加权和</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(f()\)</span></td>
<td style="text-align: center;">激活函数，可选 <span class="math inline">\(sigmoid\)</span> 或 <span class="math inline">\(tanh\)</span> 函数</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(s_l\)</span></td>
<td style="text-align: center;">第 <span class="math inline">\(l\)</span> 层的单元数目（不包含偏置单元）</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(n_l\)</span></td>
<td style="text-align: center;">神经网络层数</td>
</tr>
</tbody>
</table>
<p>有：</p>
<p><span class="math display">\[z_i^{l+1} = b_i^{l} + \sum_{j=1}^{s_{l}} W_{ij}^{l} a_j^{l}\]</span></p>
<p><span class="math display">\[a_i^l = f(z_i^l)\]</span></p>
<h3 id="前向传播forward-propagation">前向传播（forward propagation）</h3>
<p>从输入层单元开始，逐层计算下层单元，直到求得输出层。可以说就是神经网络由输入预测输出的整个过程。</p>
<h3 id="反向传播">反向传播</h3>
<p>反向传播运用梯度下降试图寻找最优模型参数，即 <span class="math inline">\(W_{ij}^l\)</span> 与 <span class="math inline">\(b_i^l\)</span>，使得代价函数最小。梯度下降开始前给参数随机赋值，用前向传播计算出各层单元。下降关键是整体代价函数 <span class="math inline">\(J\)</span> 对参数 <span class="math inline">\(W\)</span> 和 <span class="math inline">\(b\)</span> 的求导。</p>
<p>对于单个样例 <span class="math inline">\((x,y)\)</span>，其代价函数为： <span class="math display">\[ J(W,b; x,y) = \frac{1}{2} \left\| h_{W,b}(x) - y \right\|^2 \]</span></p>
<p>给定一个包含 <span class="math inline">\(m\)</span> 个样例的数据集，定义整体代价函数为：</p>
<p><span class="math display">\[  J(W,b) = \frac{1}{m} \sum_{i=1}^m J(W,b;x^{(i)},y^{(i)}) + \frac{\lambda}{2} \sum (W^{(l)}_{ji})^2 \]</span></p>
<p>第一项是对所有样例的代价求和取平均，而第二项作为规则化项，防止过拟合。因为第二项求导简单，不再赘述。对第二项只关心单个样例求导，即</p>
<p><span class="math display">\[ \frac{\partial J(W,b;x,y)}{\partial W_{ij}^l}\;\; and \;\; \frac{\partial J(W,b;x,y)}{\partial b_i^l} \]</span></p>
<p>首先有：</p>
<p><span class="math display">\[\frac{\partial J}{\partial W_{ij}^l} = \frac{\partial J}{\partial z_i^{l+1}} \frac{\partial z_i^{l+1}}{\partial W_{ij}^l} = \frac{\partial J}{\partial z_i^{l+1}} a_j^l\]</span></p>
<p>同理：</p>
<p><span class="math display">\[\frac{\partial J}{\partial b_i^l} = \frac{\partial J}{\partial z_i^{l+1}} \frac{\partial z_i^{l+1}}{\partial b_i^l} = \frac{\partial J}{\partial z_i^{l+1}} \]</span></p>
<p>所以，关键是求解：</p>
<p><span class="math display">\[  \delta^{(l+1)}_i = \frac{\partial J}{\partial z_i^{l+1}}\]</span></p>
<h4 id="当-ln_l">当 <span class="math inline">\(l=n_l\)</span></h4>
<p><span class="math display">\[  \delta^{(n_l)}_i
= \frac{\partial}{\partial z^{n_l}_i}\frac{1}{2} \left\| y - h_{W,b}(x) \right\|^2
= \frac{\partial}{\partial z^{n_l}_i}\frac{1}{2} \sum_{j=1}^{S_{n_l}} (y_j-f(z_j^{(n_l)}))^2  \\
= - (y_i - f(z_i^{(n_l)})) \cdot f&#39;(z^{(n_l)}_i) = - (y_i - a^{(n_l)}_i) \cdot f&#39;(z^{(n_l)}_i) \]</span></p>
<h4 id="当-ln_l-1">当 <span class="math inline">\(l&lt;n_l\)</span></h4>
<p><span class="math display">\[\delta^{(l)}_i = \frac{\partial J}{\partial z_i^{l}}
= \sum_{k=1}^{s_{l+1}} \frac{\partial J}{\partial z_k^{l+1}}  \frac{\partial z_k^{l+1}}{\partial a_i^l}   \frac{\partial a_i^l}{\partial z_i^l} \\
= \sum_{k=1}^{s_{l+1}} \delta^{(l+1)}_k W_{ki}^l  f&#39;(z^{(l)}_i)
\]</span></p>
<p>这里用了链式求导法则， <span class="math inline">\(a_i^l\)</span> 是 <span class="math inline">\(z_i^l\)</span> 的函数，而 <span class="math inline">\(l+1\)</span> 层的所有 <span class="math inline">\(z_k^{l+1}\)</span> 都是 <span class="math inline">\(a_i^l\)</span> 的函数且作为 <span class="math inline">\(J\)</span> 的自变量。</p>
<p>可见求解 <span class="math inline">\(\delta^{(l)}_i\)</span> 需要用到 <span class="math inline">\(l+1\)</span> 层的 <span class="math inline">\(\delta\)</span>，所以迭代过程从输出层“反向”向输入层渐进。</p>
<h3 id="自编码器与稀疏性">自编码器与稀疏性</h3>
<p>自编码器是一种输入作为输出进行训练的神经网络。通常会给隐藏层神经元添加限制，比如要求100维输入样本只能使用50个隐藏神经元，由于最终训练出的模型能“还原”输入，所以这50个隐藏神经元能“压缩”表示这100维的样本。<a href="http://deeplearning.stanford.edu/wiki/index.php/%E8%87%AA%E7%BC%96%E7%A0%81%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%80%E7%96%8F%E6%80%A7" target="_blank" rel="noopener">稀疏性</a>限制也可被用于这样的目的。</p>
<blockquote>
<p>稀疏性可以被简单地解释如下。如果当神经元的输出接近于1的时候我们认为它被激活，而输出接近于0的时候认为它被抑制，那么使得神经元大部分的时间都是被抑制的限制则被称作稀疏性限制。</p>
</blockquote>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="//schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/01/15/ML-Preprcs-NormStd/" itemprop="url">
                  数据预处理——中心化、归一化与标准化
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-01-15T12:34:08+08:00" content="2017-01-15">
              2017-01-15
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index">
                    <span itemprop="name">ML</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>参考： <a href="http://www.zhaokv.com/2016/01/normalization-and-standardization.html" target="_blank" rel="noopener">归一化与标准化</a></p>
<p>中心化（mean normalization）、归一化（normalization）和标准化（standardization）对原始数据进行处理，有利于后续的训练与分析工作。</p>
<p>中心化处理后的数据均值为 0：</p>
<p><span class="math display">\[ x&#39; = x-\mu \]</span></p>
<p>归一化将数据映射到 <code>[0, 1]</code> 之间：</p>
<p><span class="math display">\[ x&#39; = \frac{x-x_{min}}{x_{max}-x_{min}} \]</span></p>
<p>标准化处理后的数据均值为 0，方差为 1：</p>
<p><span class="math display">\[ x&#39; = \frac{x-\mu}{\sigma} \]</span></p>
<p>以上 <span class="math inline">\(\mu, \sigma\)</span> 分别为原始数据的均值与方差。</p>
<p>对于标准化和归一化，sklearn 提供了函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line">x = np.array([[ <span class="number">1.</span>, <span class="number">-1.</span>,  <span class="number">2.</span>],[ <span class="number">2.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],[ <span class="number">0.</span>,  <span class="number">1.</span>, <span class="number">-1.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 两个中心化处理调用</span></span><br><span class="line">mean_x = x - x.mean(axis=<span class="number">0</span>)</span><br><span class="line">mean_x = preprocessing.StandardScaler(with_std=<span class="keyword">False</span>).fit_transform(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 两个标准化处理调用</span></span><br><span class="line">std_x = preprocessing.scale(x)</span><br><span class="line">std_x = preprocessing.StandardScaler().fit_transform(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 归一化处理</span></span><br><span class="line">norm_x = preprocessing.MinMaxScaler().fit_transform(x)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="//schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/17/ML-MLT-10-GBDT/" itemprop="url">
                  机器学习技法第十课——Gradient Boosted Decision Tree
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-10-17T18:22:18+08:00" content="2016-10-17">
              2016-10-17
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index">
                    <span itemprop="name">ML</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>此文是本人学习<a href="http://www.csie.ntu.edu.tw/~htlin/" target="_blank" rel="noopener">林轩田老师</a>教授的机器学习技法第十一课—— Gradient Boosted Decision Tree ——的笔记。这节课会讲两个模型：Adaptive Boosted Decision Tree 和 Gradient Boosted Decision Tree (GBDT)。本质上讲，两者同属 boosting 方法结合 Decision Tree，只是他们的 error function 不同，训练方法稍有差别。（这节课有许多证明，因为太笨没懂就没详述 :-) ）</p>
<h3 id="参考">参考</h3>
<ul>
<li><a href="https://www.csie.ntu.edu.tw/~htlin/mooc/" target="_blank" rel="noopener">机器学习技法</a></li>
<li><a href="https://www.csie.ntu.edu.tw/~htlin/mooc/doc/211_handout.pdf" target="_blank" rel="noopener">课件</a></li>
<li><a href="/2016/07/28/ML-MLT-7-adaBoost/" title="机器学习技法第八课——Adaptive Boosting">机器学习技法第八课——Adaptive Boosting</a></li>
<li><a href="/2016/08/03/ML-MLT-8-decisionTree/" title="机器学习技法第九课——Decision Tree">机器学习技法第九课——Decision Tree</a></li>
<li><a href="https://en.wikipedia.org/wiki/AdaBoost" target="_blank" rel="noopener">AdaBoost - wikipedia</a></li>
<li><a href="http://suanfazu.com/t/gbdt-die-dai-jue-ce-shu-ru-men-jiao-cheng/135" target="_blank" rel="noopener">Gbdt 迭代决策树入门教程</a></li>
</ul>
<h3 id="adaptive-boosted-decision-tree">Adaptive Boosted Decision Tree</h3>
<p>Adaptive Boosted，即 AdaBoost，与 Decision Tree 已经在前面课程做过介绍，Adaptive Boosted Decision Tree 以 AdaBoost 为框架，以 Decision Tree 为弱模型，整合成一个模型。而在 <a href="/2016/07/28/ML-MLT-7-adaBoost/" title="机器学习技法第八课——Adaptive Boosting">机器学习技法第八课——Adaptive Boosting</a> 中，弱模型简单地选取 Decision Stump，这也算 Adaptive Boosted Decision Tree 的一个特例，老师称之为 <strong>AdaBoost-Stump</strong>。</p>
<h4 id="模型训练">模型训练</h4>
<p>在训练过程中，应该注意两点。</p>
<p>一、决策树应该剪枝。一方面为了正则化，另一方面，完全决策树的误差为零，这使得以修正误差为中心的 AdaBoost 难以进行。</p>
<p>二、从整个训练集中抽样出每次迭代的训练数据，每笔数据的选中概率正比于 AdaBoost 所计算的权重，替代了原来（第八课）将权重直接用于 error function 的方式。一方面与剪枝目的相同，避免将所有数据输入完全树导致误差为零。另一方面，权重不仅能通过抽取概率体现数据的重要程度，同时又能避免对弱模型 error function 的修改。</p>
<p>训练过程可参考第八课，改动的只有：在训练弱模型时，根据权重随机抽取训练数据，并训练出剪枝的决策树。</p>
<h4 id="adaboost-与梯度下降">AdaBoost 与梯度下降</h4>
<p>课上，老师详细证明了 AdaBoost 的训练过程是在最小（优）化函数，所谓的 loss function ：</p>
<p><span class="math display">\[ loss = \frac{1}{N} \sum_{n=1}^N \exp (-y_n s_n^{(T)}) \]</span></p>
<p>其中 <span class="math inline">\(N\)</span> 表示数据量， <span class="math inline">\(T\)</span> 表示迭代次数， <span class="math inline">\(s_n^{(T)}\)</span> 表示 <span class="math inline">\(T\)</span> 次迭代后模型估计值：</p>
<p><span class="math display">\[ s_n^{(T)} = \sum_{t=1}^T\alpha_t g_t(\mathbf{x}_n) \]</span></p>
<p>类似梯度下降，AdaBoost 的每次迭代都向最优点靠近，最终到达最优点。AdaBoost 的正向推导可参考：<a href="https://en.wikipedia.org/wiki/AdaBoost" target="_blank" rel="noopener">AdaBoost - wikipedia</a>。</p>
<h3 id="gradient-boosted-decision-tree">Gradient Boosted Decision Tree</h3>
<p>简称 GBDT，它的思想与 AdaBoost 是相同的，只是 loss function 被替换成其他函数。流行用残差平方函数替换 AdaBoost 中的 <span class="math inline">\(\exp\)</span> 函数，以下只介绍这种 GBDT。</p>
<p><span class="math display">\[ loss=\frac{1}{N} \sum_{n=1}^N (s_n^{(T)} - y_n)^2 \]</span></p>
<h4 id="模型训练-1">模型训练</h4>
<p>GBDT 的训练流程与 AdaBoost 稍有不同。</p>
<p>每次迭代根据上次估计值与实际值的残差（residual）训练决策树，不断减小残差使估计值逼近实际值。参考博文 <a href="http://suanfazu.com/t/gbdt-die-dai-jue-ce-shu-ru-men-jiao-cheng/135" target="_blank" rel="noopener">Gbdt 迭代决策树入门教程</a>，它详细介绍了 GBDT 所隐含的直觉。</p>
<p>每次迭代的权重计算公式：</p>
<p><span class="math display">\[ \alpha_t = \frac{\sum_{n=1}^N g_t(\mathbf{x}_n)(y_n-s_n^{(t-1)})}{\sum_{n=1}^N g_t^2(\mathbf{x}_n)} \]</span></p>
<p>其中 <span class="math inline">\(s_n^{(t-1)}\)</span> 表示 <span class="math inline">\(t-1\)</span> 次迭代（前次迭代）后模型估计值。</p>
<ul>
<li><span class="math inline">\(s_1=s_2=...=s_n\)</span></li>
<li>for t=1,2,…,T
<ul>
<li>根据数据集 <span class="math inline">\(\{(\mathbf{x}_n, y_n-s_n)\}\)</span>，训练出决策树 <span class="math inline">\(g_t\)</span></li>
<li>根据数据集 <span class="math inline">\(\{(g_t(\mathbf{x}_n),y_n-s_n)\}\)</span> 计算权重 <span class="math inline">\(\alpha_t\)</span></li>
<li>更新 <span class="math inline">\(s_n \leftarrow s_n + \alpha_t g_t(\mathbf{x}_n)\)</span></li>
</ul></li>
<li>返回模型 <span class="math inline">\(G(\mathbf{x})=\sum\alpha_t g_t(\mathbf{x}_n)\)</span></li>
</ul>
<p>注，在一些实现中，权重 <span class="math inline">\(\alpha_t\)</span> 直接设定为 1，没有经过计算。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="//schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/28/Math-LinAlg-analyticGeometry/" itemprop="url">
                  空间解析几何
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-09-28T22:10:50+08:00" content="2016-09-28">
              2016-09-28
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Math/" itemprop="url" rel="index">
                    <span itemprop="name">Math</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a href="/2016/05/21/Math-LinAlg-Matrix/" title="矩阵相关">矩阵相关</a>
<p><br>
<a href="/2016/05/23/Math-LinAlg-hyperplane/" title="超平面">超平面</a></p>
<h3 id="平面">平面</h3>
<p>平面有两个自由度</p>
<h4 id="平面表示方法">平面表示方法</h4>
<h5 id="点法式方程">点法式方程</h5>
<p><span class="math display">\[ a(x-x_0) + b(y-y_0) + c(z-z_0) = 0 \]</span></p>
<p>其中 <span class="math inline">\(M_0(x_0,y_0,z_0)\)</span> 为平面上一点， <span class="math inline">\(\mathbf{n}=(a,b,c)\)</span> 为平面法向量</p>
<h5 id="一般式方程">一般式方程</h5>
<p><span class="math display">\[ ax + by + cz + d = 0 \]</span></p>
<p>其中 <span class="math inline">\(\mathbf{n}=(a,b,c)\)</span> 为平面法向量。若 <span class="math inline">\(d=0\)</span>，平面过原点；若 <span class="math inline">\(a=0\)</span> 且 <span class="math inline">\(d\ne0\)</span>， 法向量正交 x 轴，故平面平行于 x 轴。</p>
<h4 id="平面间位置关系">平面间位置关系</h4>
<ul>
<li>平行 <span class="math inline">\(\frac{a_1}{a_2}=\frac{b_1}{b_2}=\frac{c_1}{c_2}\ne\frac{d_1}{d_2}\)</span></li>
<li>重合 <span class="math inline">\(\frac{a_1}{a_2}=\frac{b_1}{b_2}=\frac{c_1}{c_2}=\frac{d_1}{d_2}\)</span></li>
<li>相交 <span class="math inline">\(\frac{a_1}{a_2}=\frac{b_1}{b_2}=\frac{c_1}{c_2}\)</span> 不成立</li>
</ul>
<h4 id="点到平面距离">点到平面距离</h4>
<p>点 <span class="math inline">\((x_0,y_0,z_0)\)</span>， 平面 <span class="math inline">\(Ax+By+Cz+D=0\)</span></p>
<p><span class="math display">\[ d=\frac{|Ax_0+By_0+Cz_0+D|}{\sqrt{A^2+B^2+C^2}} \]</span></p>
<h3 id="直线">直线</h3>
<p>直线只有一个自由度</p>
<h4 id="直线表示方法">直线表示方法</h4>
<h5 id="点向式方程">点向式方程</h5>
<p><span class="math display">\[ \frac{x-x_0}{m}=\frac{y-y_0}{n}=\frac{z-z_0}{p} \]</span></p>
<p>其中 <span class="math inline">\(M_0(x_0,y_0,z_0)\)</span> 为直线上一点， <span class="math inline">\(\mathbf{s}=(m,n,p)\)</span> 为直线的 <strong>方向向量</strong>。 如果 <span class="math inline">\(m=0\)</span>， 应该转化为 <span class="math inline">\(x=x_0\)</span>。</p>
<h5 id="参数式方程">参数式方程</h5>
<p><span class="math display">\[ \left\{ \begin{array}{l}
x=\lambda m + x_0\\
y=\lambda n + y_0\\
z=\lambda p + z_0
\end{array} \right. \]</span></p>
<p>其中 <span class="math inline">\(M_0(x_0,y_0,z_0)\)</span> 为直线上一点， <span class="math inline">\(\mathbf{s}=(m,n,p)\)</span> 为直线的 <strong>方向向量</strong>。</p>
<h5 id="一般式方程-1">一般式方程</h5>
<p><span class="math display">\[ \left\{ \begin{array}{l}
a_1 x+ b_1 y + c_1 z+ d_1 = 0\\
a_2 x+ b_2 y + c_2 z+ d_2 = 0
\end{array} \right. \]</span></p>
<p>两平面相交的直线方程。</p>
<h4 id="直线间位置关系">直线间位置关系</h4>
<p><span class="math inline">\(\mathbf{s}_1\)</span> 与 <span class="math inline">\(\mathbf{s}_2\)</span> 分别为两直线的方向向量， <span class="math inline">\(M_1\)</span> 与 <span class="math inline">\(M_2\)</span> 分别为其上两点。</p>
<ul>
<li>两直线平行 <span class="math inline">\(\Leftrightarrow\)</span> 方向向量平行且不平行于 <span class="math inline">\({M_1M_2}\)</span></li>
<li>两直线重合 <span class="math inline">\(\Leftrightarrow\)</span> 方向向量平行且平行于 <span class="math inline">\({M_1M_2}\)</span></li>
<li>两直线相交 <span class="math inline">\(\Leftrightarrow\)</span> 方向向量不平行且与 <span class="math inline">\({M_1M_2}\)</span> 混合积为零（共面）</li>
<li>两直线异面 <span class="math inline">\(\Leftrightarrow\)</span> 方向向量不平行且与 <span class="math inline">\({M_1M_2}\)</span> 混合积不为零（非共面）</li>
</ul>
<h4 id="直线与平面的位置关系">直线与平面的位置关系</h4>
<ul>
<li>平行 <span class="math inline">\(\Leftrightarrow\)</span> 方向向量与平面法向量正交，且直线至少有一点不在平面上</li>
<li>直线在平面上 <span class="math inline">\(\Leftrightarrow\)</span> 方向向量与平面法向量正交，且直线至少有一点在平面上</li>
<li>相交 <span class="math inline">\(\Leftrightarrow\)</span> 方向向量与平面法向量非正交</li>
</ul>
<p>对直线：</p>
<p><span class="math display">\[ \left\{ \begin{array}{l}
a_1 x+ b_1 y + c_1 z+ d_1 = 0\\
a_2 x+ b_2 y + c_2 z+ d_2 = 0
\end{array} \right. \]</span></p>
<p>则过直线的平面方程可表示为：</p>
<p><span class="math display">\[ \alpha(a_1 x+ b_1 y + c_1 z+ d_1) + \beta (a_2 x+ b_2 y + c_2 z+ d_2) = 0 \]</span></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="//schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/26/Math-Calculus-multiDerivat/" itemprop="url">
                  多元函数最值
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-09-26T09:53:31+08:00" content="2016-09-26">
              2016-09-26
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Math/" itemprop="url" rel="index">
                    <span itemprop="name">Math</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a href="/2016/06/30/Math-Calculus-cheatSheet/" title="微积分笔记">微积分笔记</a>
<h3 id="多元函数极值">多元函数极值</h3>
<h4 id="存在的必要条件">存在的必要条件</h4>
<p>多元函数在极值点处若存在偏导数，则必为零。</p>
<h4 id="存在的充分条件">存在的充分条件</h4>
<p>若 <span class="math inline">\(f(x,y)\)</span> 在 <span class="math inline">\((x_0, y_0)\)</span> 存在二阶偏导，一阶偏导为零， <strong>黑塞矩阵</strong> 为：</p>
<p><span class="math display">\[ H = \begin{bmatrix}
        f_{xx} &amp; f_{xy} \\
        f_{yx} &amp; f_{yy} \\
        \end{bmatrix}
\]</span></p>
<p>则有以下结论：</p>
<ul>
<li>若 <span class="math inline">\(f_{xx}f_{yy}-f_{xy}f_{yx}&gt;0\)</span> 且 <span class="math inline">\(f_{xx}&gt;0\)</span>， 则 <span class="math inline">\(H\)</span> 为正定矩阵，故 <span class="math inline">\(f(x_0,y_0)\)</span> 为极小值</li>
<li>若 <span class="math inline">\(f_{xx}f_{yy}-f_{xy}f_{yx}&gt;0\)</span> 且 <span class="math inline">\(f_{xx}&lt;0\)</span>， 则 <span class="math inline">\(H\)</span> 为负定矩阵，故 <span class="math inline">\(f(x_0,y_0)\)</span> 为极大值</li>
<li>若 <span class="math inline">\(f_{xx}f_{yy}-f_{xy}f_{yx}&lt;0\)</span>， 则 <span class="math inline">\(H\)</span> 为不定矩阵，故 <span class="math inline">\(f(x_0,y_0)\)</span> 不是极值</li>
</ul>
<h3 id="有界闭区域上的最值">有界闭区域上的最值</h3>
<p><span class="math inline">\(f(x,y)\)</span> 在某一有界闭区域上连续，则 <span class="math inline">\(f(x,y)\)</span> 必定有最大最小值。最值可能是区域内极值及边界上的最值。求最值的一般步骤：</p>
<ol type="1">
<li>求 <span class="math inline">\(f(x,y)\)</span> 在区域内所有驻点（偏导为零）及其函数值</li>
<li>求 <span class="math inline">\(f(x,y)\)</span> 在边界上的最值</li>
<li>比较前两步的函数值，得出最值</li>
</ol>
<p>求边界上的最值，直接将边界函数代入目标函数或使用拉格朗日乘数法。</p>
<h3 id="条件极值与拉格朗日乘数法">条件极值与拉格朗日乘数法</h3>
<p>求目标函数极值： <span class="math inline">\(u=f(x,y)\)</span><br>
约束函数： <span class="math inline">\(\varphi(x,y)=0\)</span><br>
拉格朗日函数： <span class="math inline">\(F(x,y,\lambda)=f(x,y)+\lambda\varphi(x,y)\)</span></p>
<p>目标函数极值点满足（必要条件）拉格朗日函数偏导为零：</p>
<p><span class="math display">\[ \left\{ \begin{array}{l}
F_x = 0 \\
F_y = 0 \\
F_\lambda = \varphi(x,y) = 0
\end{array} \right. \]</span></p>
<p>可推广到多元函数多约束函数情况。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="//schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/21/Math-PrStats-estimationTesting/" itemprop="url">
                  参数估计与假设检验
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-09-21T17:16:07+08:00" content="2016-09-21">
              2016-09-21
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Math/" itemprop="url" rel="index">
                    <span itemprop="name">Math</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a href="/2016/09/02/Math-PrStats-cheatSheet/" title="概率论与数理统计">概率论与数理统计</a>
<p><br>
<a href="/2016/09/21/Math-PrStats-statistics/" title="数理统计基本概念">数理统计基本概念</a></p>
<p>参考：<a href="http://support.minitab.com/zh-cn/minitab/17/topic-library/basic-statistics-and-graphs/introductory-concepts/p-value-and-significance-level/significance-level/" target="_blank" rel="noopener">我应该对显著性水平使用什么值？</a></p>
<p><strong>参数估计</strong> 指用样本估计总体分布的参数。分点估计和区间估计。<br>
<strong>假设检验</strong> 指对总体情况的假设做检验，在给定显著水平下做出拒绝或接受的判断。</p>
<h3 id="参数的点估计">参数的点估计</h3>
<p>构造一个适当的统计量 <span class="math inline">\(\hat{\theta}=\hat{\theta}(X_1,X_2,...,X_n)\)</span>， 作为对总体参数 <span class="math inline">\(\theta\)</span> 的估计。称函数（也是随机变量） <span class="math inline">\(\hat{\theta}\)</span> 为参数 <span class="math inline">\(\theta\)</span> 的 <strong>估计量</strong>， 代入样本观测值得到函数值，称为 <strong>估计值</strong>。</p>
<h4 id="矩估计">矩估计</h4>
<p>矩估计的依据：大数定律</p>
<p>用总体矩表示总体参数，然后用对应样本矩作为总体矩的估计，达到以样本矩估计总体参数的目的。以下用矩估计期望 <span class="math inline">\(\mu\)</span> 和二阶中心矩 <span class="math inline">\(M_2\)</span>：</p>
<p><span class="math display">\[ \hat{\mu} = \overline{X} = \frac{1}{n} \sum_{i=1}^n X_i \\
   \hat{\sigma}^2 = M_2 = \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2
\]</span></p>
<h4 id="极大似然估计">极大似然估计</h4>
<p>按照最大可能性估计参数。构造抽出当前样本的概率——<strong>似然函数</strong>，（常用 <strong>似然方程</strong>）估计最优参数使该概率最大。</p>
<p>似然函数：样本个体概率之积（离散型）或概率密度之积（连续型），记为 <span class="math inline">\(L(x_1,x_2,...,x_n;\theta_1,\theta_2,...,\theta_m)\)</span>；<br>
似然方程：似然函数的对数对参数求偏导</p>
<p><span class="math display">\[ \frac{\partial\ln L}{\partial\theta_k},\quad k=1,2,...,m\]</span></p>
<h3 id="估计量的优良性准则">估计量的优良性准则</h3>
<p>不同方法求出的估计量很可能不同，估计量的优良性准则就可用于挑选优良估计量。</p>
<h4 id="无偏性">无偏性</h4>
<p>如果 <span class="math inline">\(E(\hat{\theta})=\theta\)</span>， 则称 <span class="math inline">\(\hat{\theta}\)</span> 为 <strong>无偏估计量</strong>。 样本方差是总体方差的无偏估计。</p>
<h4 id="有效性">有效性</h4>
<p>如果无偏估计 <span class="math inline">\(\hat{\theta}_1\)</span> 和 <span class="math inline">\(\hat{\theta}_2\)</span> 满足 <span class="math inline">\(D(\hat{\theta}_1) \le D(\hat{\theta}_2)\)</span>， 则称 <span class="math inline">\(\hat{\theta}_1\)</span> 比 <span class="math inline">\(\hat{\theta}_2\)</span> <strong>有效</strong>。如果 <span class="math inline">\(\hat{\theta}\)</span> 是 <span class="math inline">\(\theta\)</span> 估计中方差最小的，称为 <strong>最小方差无偏估计</strong>。</p>
<h4 id="相合性">相合性</h4>
<p>估计量应随着样本容量增大而接近总体参数的真值。当样本容量 <span class="math inline">\(n\rightarrow \infty\)</span> 时， <span class="math inline">\(\hat{\theta}_n\)</span> 依概率收敛于 <span class="math inline">\(\theta\)</span>， 即对任意 <span class="math inline">\(\epsilon &gt; 0\)</span>， 满足</p>
<p><span class="math display">\[ \lim_{n\rightarrow \infty} P\{ |\hat{\theta}_n - \theta| &lt; \epsilon \} = 1 \]</span></p>
<p>则称 <span class="math inline">\(\hat{\theta}_n\)</span> 为 <span class="math inline">\(\theta\)</span> 的 <strong>相合估计量</strong> 或 <strong>一致估计量</strong>。</p>
<h3 id="区间估计">区间估计</h3>
<p>点估计不能给出估计值的可信程度，区间估计弥补了这点。区间估计指估计参数的取值区间及对应的可信程度（概率）。它有 2 个基本要求，可信程度（概率）尽可能大和区间尽可能小，显然这是矛盾的。一般的做法是先指定可信程度，再在该前提下尽可能缩小区间。</p>
<p>如果满足</p>
<p><span class="math display">\[ P\{\hat{\theta}_1(X_1,X_2,...,X_n) \le \theta \le \hat{\theta}_2(X_1,X_2,...,X_n)\} = 1 -\alpha \]</span></p>
<p>则称 <span class="math inline">\([\hat{\theta}_1,\hat{\theta}_2]\)</span> 为 <span class="math inline">\(\theta\)</span> 的 <strong>置信度</strong> 为 <span class="math inline">\(1-\alpha\)</span> 的 <strong>置信区间</strong>。</p>
<h4 id="枢轴变量法">枢轴变量法</h4>
<p>在 <a href="/2016/09/21/Math-PrStats-statistics/" title="几个抽样分布定理">几个抽样分布定理</a> 中，可见 <strong>统计量、总体分布参数与抽样分布</strong> 三者存在一定关系，利用这些关系构造“统计量与总体参数的函数”——服从已知分布的 <strong>枢轴变量</strong>， 及其指定置信度下的概率方程，解出枢轴变量估计区间，进而解出参数估计区间。</p>
<p>例如，已知 <strong>正态</strong> 总体分布方差 <span class="math inline">\(\sigma^2\)</span> 和一个容量为 n 的样本，给定置信度 <span class="math inline">\(1-\alpha\)</span>， 求期望 <span class="math inline">\(\mu\)</span>，可构造枢轴变量</p>
<p><span class="math display">\[ U = \frac{\overline{X} - \mu}{\sigma / \sqrt{n}} \sim N(0,1),\quad because\; \overline{X} \sim N(\mu,\frac{\sigma^2}{n})  \]</span></p>
<p><span class="math display">\[ \Rightarrow P\{ -u_{\alpha/2} \le U \le u_{\alpha/2}\} = 1-\alpha \]</span></p>
<p>其中 <span class="math inline">\(u_{\alpha/2}\)</span> 是正态分布 <strong>上侧 <span class="math inline">\(\frac{\alpha}{2}\)</span> 分位数</strong>， 可以查表解出得到 <span class="math inline">\(U\)</span> 的估计区间，进而解聘 <span class="math inline">\(\mu\)</span> 的估计区间。</p>
<h4 id="一个正态总体的区间估计">一个正态总体的区间估计</h4>
<p>以下是用枢轴变量法解出的正态总体参数的估计区间结果。 <span class="math inline">\(\mu，\sigma^2\)</span> 分别为期望、方差，指定置信度为 <span class="math inline">\(1-\alpha\)</span>。</p>
<h5 id="mu-置信区间"><span class="math inline">\(\mu\)</span> 置信区间</h5>
<p><span class="math display">\[ [\overline{X}-\frac{\sigma}{\sqrt{n}}u_{\alpha/2},\;\; \overline{X}+\frac{\sigma}{\sqrt{n}}u_{\alpha/2}],\quad with\; \sigma^2\; known\]</span></p>
<p><span class="math display">\[ [\overline{X}-\frac{S}{\sqrt{n}}t_{\alpha/2}(n-1),\;\; \overline{X}+\frac{S}{\sqrt{n}}t_{\alpha/2}(n-1)],\quad with\; \sigma^2\; unknown\]</span></p>
<p>其中 <span class="math inline">\(u_{\alpha/2},\;t_{\alpha/2}\)</span> 分别是正态分布与 T 分布的上侧分位数。</p>
<h5 id="sigma2-置信区间"><span class="math inline">\(\sigma^2\)</span> 置信区间</h5>
<p><span class="math display">\[ [\frac{(n-1)S^2}{\chi^2_{\alpha/2}(n-1)} ,\;\; \frac{(n-1)S^2}{\chi^2_{1-\alpha/2}(n-1)}] ,\quad with\; \mu\; unknown \]</span></p>
<p>其中 <span class="math inline">\(\chi^2_{\alpha/2},\;\chi^2_{1-\alpha/2}\)</span> 为 <span class="math inline">\(\chi^2\)</span> 上侧分位数。</p>
<h4 id="两个正态总体的区间估计">两个正态总体的区间估计</h4>
<p>以下是用枢轴变量法解出的正态总体参数的区间估计。 <span class="math inline">\(\mu_1，\sigma_1^2\)</span> 与 <span class="math inline">\(\mu_2，\sigma_2^2\)</span> 分别为两个总体的期望、方差，指定置信度为 <span class="math inline">\(1-\alpha\)</span>。</p>
<h5 id="mu_1---mu_2"><span class="math inline">\(\mu_1 - \mu_2\)</span></h5>
<p><span class="math display">\[ [\overline{X} - \overline{Y} - u_{\alpha/2}\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}} ,\;\; \overline{X} - \overline{Y} + u_{\alpha/2}\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}} ],\quad with\; \sigma_1^2,\sigma_2^2\; known\]</span></p>
<p>如果 <span class="math inline">\(\sigma_1^2,\sigma_2^2\)</span> 未知但相等，则区间为</p>
<p><span class="math display">\[ [(\overline{X}-\overline{Y})-t_{\alpha/2}(n_1+n_2-2)S_w\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}  ,\quad  (\overline{X}-\overline{Y})+t_{\alpha/2}(n_1+n_2-2)S_w\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}] \]</span></p>
<p><span class="math display">\[ S_w = \sqrt{\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{n_1+n_2-2}} \]</span></p>
<p>其中 <span class="math inline">\(u_{\alpha/2},\;t_{\alpha/2}\)</span> 分别是正态分布与 T 分布的上侧分位数。</p>
<h5 id="fracsigma_12sigma_22-区间"><span class="math inline">\(\frac{\sigma_1^2}{\sigma_2^2}\)</span> 区间</h5>
<p><span class="math display">\[ [\frac{S_2^2}{S_1^2}F_{1-\alpha/2}(n_1-1,n_2-1),\;\;  \frac{S_2^2}{S_1^2}F_{\alpha/2}(n_1-1,n_2-1)],\quad with\; \mu_1,\mu_2\; unknown\]</span></p>
<p>其中 <span class="math inline">\(F_{\alpha/2},F_{1-\alpha/2}\)</span> 为 F 分布上侧分位数。</p>
<h4 id="大样本方法">大样本方法</h4>
<p>用“样本均值”与“总体期望方差”构造出枢轴变量，使之满足中心极限定理，当样本容量足够大时，近似服从正态分布，从而得到指定置信度下的概率方程，解出置信区间。如果总体期望方差均未知，可用点估计代替其中一个解另一个。</p>
<h4 id="单侧置信区间">单侧置信区间</h4>
<p>指估计区间只要求上限或下限，与双侧置信区间解法区别不大。</p>
<h3 id="假设检验">假设检验</h3>
<p>为了检验假设 <span class="math inline">\(H_0\)</span>， 称为 <strong>原假设</strong> 或 <strong>零假设</strong>， 假定 <span class="math inline">\(H_0\)</span> 是正确的，进行推导，如果指导出小概率事件，就有理由拒绝原假设，反之，接受原假设。</p>
<p>原假设的逆命题 <span class="math inline">\(H_1\)</span> 称为 <strong>对立假设</strong> 或 <strong>备择假设</strong>。 使原假设接受的（根据假设构造的） <strong>检验统计量</strong> 取值区域称为 <strong>检验的接受域</strong>， 反之为 <strong>检验的拒绝域</strong>。</p>
<h4 id="假设检验的两类错误">假设检验的两类错误</h4>
<p>假设检验可能犯两类错误， <strong>第一类错误“弃真”</strong> 和 <strong>第二类错误“纳伪”</strong>。 犯第一类错误的概率：</p>
<p><span class="math display">\[ P\{ 拒绝 H_0 | H_0 为真\} = \alpha \]</span></p>
<p>称为 <strong>显著性水平</strong>， 通常是一个 <strong>小概率</strong>。 <strong>第一二类错误概率发生概率是互相矛盾的</strong>， 在实际应用中，根据二者犯错代价设置显著性水平大小。（参考：<a href="http://support.minitab.com/zh-cn/minitab/17/topic-library/basic-statistics-and-graphs/introductory-concepts/p-value-and-significance-level/significance-level/" target="_blank" rel="noopener">我应该对显著性水平使用什么值？</a>）</p>
<h4 id="显著性检验的步骤">显著性检验的步骤</h4>
<p>整个过程类似参数区间估计</p>
<ul>
<li>提出假设</li>
<li>构造检验统计量，并 <strong>在 <span class="math inline">\(H_0\)</span> 成立的条件下</strong>， 能确定检验统计量的分布</li>
<li>选定显著性水平 <span class="math inline">\(\alpha\)</span>， 确定拒绝域</li>
<li>根据样本观测值和拒绝域，作出检验决策</li>
</ul>
<h4 id="单个正态总体假设检验">单个正态总体假设检验</h4>
<h5 id="期望-mu-检验">期望 <span class="math inline">\(\mu\)</span> 检验</h5>
<p><span class="math inline">\(H_0: \mu=\mu_0,\quad H_1: \mu \ne \mu_0\)</span></p>
<p><strong>总体方差 <span class="math inline">\(\sigma^2\)</span> 已知</strong>， 检验统计量：</p>
<p><span class="math display">\[ U = \frac{\overline{X} - \mu_0}{\sigma/\sqrt{n}} \sim N(0,1),\quad 拒绝域：|u|&gt; u_{\alpha/2} \]</span></p>
<p><strong>总体方差未知</strong>， 检验统计量：</p>
<p><span class="math display">\[ T = \frac{\overline{X} - \mu_0}{S/\sqrt{n}} \sim t(n-1),\quad 拒绝域：|t|&gt; t_{\alpha/2}(n-1) \]</span></p>
<p>其中， <span class="math inline">\(u_{\alpha/2},\;t_{\alpha/2}(n-1)\)</span> 为正态分布与 T 分布上侧分位数。对于备择假设为不等式的情况，统计量不变，拒绝域稍做变化。</p>
<h5 id="方差-sigma2-检验">方差 <span class="math inline">\(\sigma^2\)</span> 检验</h5>
<p><span class="math inline">\(H_0: \sigma^2=\sigma^2_0,\quad H_1: \sigma^2 \ne \sigma^2_0\)</span></p>
<p><strong>总体期望未知</strong>，检验统计量：</p>
<p><span class="math display">\[ \chi^2 = (n-1)\frac{S^2}{\sigma_0^2} \sim \chi^2(n-1),\quad 拒绝域：\chi^2 &gt; \chi_{\alpha/2}^2(n-1)\; 或\; \chi^2 &lt; \chi_{1-\alpha/2}^2(n-1) \]</span></p>
<p><strong>总体期望 <span class="math inline">\(\mu\)</span> 已知</strong>，检验统计量：</p>
<p><span class="math display">\[ \chi^2 = \frac{\sum_{i=1}^n (X_i - \mu)^2}{\sigma_0^2} \sim \chi^{2}(n),\quad 拒绝域：\chi^2 &gt; \chi_{\alpha/2}^2(n)\; 或\; \chi^2 &lt; \chi_{1-\alpha/2}^2(n)  \]</span></p>
<p>参考“期望 <span class="math inline">\(\mu\)</span> 检验”备注。</p>
<h4 id="两个正态总体假设检验">两个正态总体假设检验</h4>
<h5 id="期望差-mu_1---mu_2-检验">期望差 <span class="math inline">\(\mu_1 - \mu_2\)</span> 检验</h5>
<p><span class="math inline">\(H_0: \mu_1 - \mu_2 = 0,\quad H_1: \mu_1 - \mu_2 \ne 0\)</span></p>
<p><strong>总体方差 <span class="math inline">\(\sigma_1^2,\;\sigma_2^2\)</span> 均已知</strong>， 检验统计量：</p>
<p><span class="math display">\[ U=\cfrac{(\overline{X}-\overline{Y})}{\sqrt{\sigma_1^2/n_1+\sigma_2^2/n_2}} \sim N(0,1),\quad 拒绝域：|u|&gt;u_{\alpha/2}\]</span></p>
<p><strong>总体方差 <span class="math inline">\(\sigma_1^2,\;\sigma_2^2\)</span> 均未知，但 <span class="math inline">\(\sigma_1^2=\sigma_2^2=\sigma\)</span></strong>， 检验统计量：</p>
<p><span class="math display">\[ T=\cfrac{(\overline{X}-\overline{Y})}{S_w\sqrt{1/n_1+1/n_2}} \sim t(n_1+n_2-2),\quad 拒绝域：|t|&gt;t_{\alpha/2}(n_1+n_2-2)\]</span></p>
<p><span class="math display">\[ S_w = \sqrt{\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{n_1+n_2-2}} \]</span></p>
<p>参考“期望 <span class="math inline">\(\mu\)</span> 检验”备注。</p>
<h5 id="方差比-fracsigma_12sigma_12-检验">方差比 <span class="math inline">\(\frac{\sigma_1^2}{\sigma_1^2}\)</span> 检验</h5>
<p><span class="math inline">\(H_0:\sigma_1^2=\sigma_2^2,\quad H_1:\sigma_1^2\ne\sigma_2^2\)</span></p>
<p><strong>总体期望均未知</strong>， 检验统计量：</p>
<p><span class="math display">\[ F=\frac{S_1^2}{S_2^2} \sim F(n_1-1, n_2-2),\quad 拒绝域：f &gt; F_{\alpha/2}(n_1-1,n_2-1)\; 或\; f &lt; F_{1-\alpha/2}(n_1-1,n_2-1) \]</span></p>
<p>参考“期望 <span class="math inline">\(\mu\)</span> 检验”备注。</p>
<h4 id="大样本检验法">大样本检验法</h4>
<p>类比区间估计的大样本方法</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="//schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/21/Math-PrStats-statistics/" itemprop="url">
                  数理统计基本概念
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-09-21T11:23:08+08:00" content="2016-09-21">
              2016-09-21
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Math/" itemprop="url" rel="index">
                    <span itemprop="name">Math</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a href="/2016/09/02/Math-PrStats-cheatSheet/" title="概率论与数理统计">概率论与数理统计</a>
<p><br>
<a href="/2016/09/21/Math-PrStats-estimationTesting/" title="参数估计与假设检验">参数估计与假设检验</a></p>
<h3 id="基本概念">基本概念</h3>
<h4 id="样本与样本观测值">样本与样本观测值</h4>
<p>样本是由 <strong>总体</strong> 中的一部分 <strong>个体</strong> 组成，是一组随机变量，记为 <span class="math inline">\(X_1,\,X_2,...,X_n\)</span>， 其中 n 为 <strong>样本容量</strong>。样本中个体的实际取值 <span class="math inline">\(x_1,\,x_2,...,x_n\)</span> 称为 <strong>样本观测值</strong>。</p>
<h4 id="统计量与统计值">统计量与统计值</h4>
<p><strong>统计量</strong> 是不含未知参数的样本函数 <span class="math inline">\(g(X_1,\,X_2,\,...,X_n)\)</span>， 也是一个随机变量。统计量是对总体进行推断的基础。 <strong>统计值</strong> 是样本观测值代入统计量中得到的函数值。</p>
<h4 id="抽样分布">抽样分布</h4>
<p>统计量的分布</p>
<h3 id="常用统计量">常用统计量</h3>
<p><span class="math display">\[  样本均值\, —— \, \overline{X} = \frac{1}{n} \sum_{i=1}^n X_i \\
    样本方差\, —— \, S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2 \\
    样本标准差\, —— \, S\\
    样本\, k\, 阶原点矩\, —— \, A_k = \frac{1}{n} \sum_{i=1}^n X_i^k \\
    样本\, k\, 阶中心矩\,—— \, M_k = \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^k
\]</span></p>
<h3 id="抽样分布-1">抽样分布</h3>
<h4 id="chi2-分布"><span class="math inline">\(\chi^2\)</span> 分布</h4>
<p>若随机变量 <span class="math inline">\(X\)</span> 的概率密度函数为： <span class="math display">\[ f(x) = \left\{\begin{array}{}
\cfrac{1}{2\Gamma(\frac{n}{2})} (\frac{x}{2})^{\frac{n}{2}-1} e^{-\frac{x}{2}},\quad x&gt;0\\
0,\quad x \le 0
\end{array}\right. \]</span></p>
<p>则称 <span class="math inline">\(X\)</span> <strong>服从自由度为 n 的 <span class="math inline">\(\chi^2\)</span> 分布</strong>， 记 <span class="math inline">\(X\sim \chi^2(n)\)</span>。 其中，</p>
<p><span class="math display">\[ \Gamma(s) = \int_0^{+\infty} x^{s-1}e^{-x}\,dx\]</span></p>
<h5 id="性质">性质</h5>
<p><span class="math display">\[  X \sim \chi^2(n) \Rightarrow E(X) = n,\;  D(X)=2n\\
    X,\;Y\;相互独立，且\;X\sim\chi^2(n_1),\;Y\sim\chi^2(n_2) \Rightarrow X+Y\sim\chi^2(n_1+n_2)\\
    \chi_\alpha^2(n) \approx n+u_\alpha\sqrt{2n},\quad with\; big\; n\\
    \chi^2(n)\;近似服从\;N(n,2n) ,\quad with\; big\; n
\]</span> 在第三条性质中， <span class="math inline">\(\chi_\alpha^2(n)\)</span> 为上侧概率为 <span class="math inline">\(\alpha\)</span> 的 <span class="math inline">\(\chi^2(n)\)</span> 上侧分位数， <span class="math inline">\(u_\alpha\)</span> 为上侧概率为 <span class="math inline">\(\alpha\)</span> 的标准正态分布的上侧分位数。</p>
<h4 id="t-分布"><span class="math inline">\(t\)</span> 分布</h4>
<p>随机变量 T 的概率密度函数为：</p>
<p><span class="math display">\[ f(x) = \cfrac{\Gamma(\frac{n+1}{2})}{\sqrt{n\pi}\Gamma(\frac{n}{2})} (1+\frac{x^2}{n})^{-\frac{n+1}{2}},\quad -\infty&lt;x&lt;+\infty \]</span></p>
<p>则称 T <strong>服从自由度为 n 的 t 分布</strong>， 记 <span class="math inline">\(T \sim t(n)\)</span></p>
<p>当 n 足够大时， T 近似服从 <span class="math inline">\(N(0,1)\)</span> 分布。</p>
<h4 id="f-分布"><span class="math inline">\(F\)</span> 分布</h4>
<p>设随机变量 F 的概率密度函数为</p>
<p><span class="math display">\[ f(x) = \left\{\begin{array}{}
  n_1^{n_1/2} n_2^{n_2/2} \cfrac{\Gamma((n_1+n_2)/2)}{\Gamma(n_1 / 2)\Gamma(n_2/2)} x^{n_1/2-1}(n_1 x + n_2)^{(n_1+n_2)/2},\quad x &gt; 0\\
  0,\quad x \le 0
  \end{array}\right. \]</span></p>
<p>则称 F <strong>服从第一自由度为 <span class="math inline">\(n_1\)</span>， 服从第二自由度为 <span class="math inline">\(n_2\)</span> 的 F 分布</strong>， 记为 <span class="math inline">\(F\sim F(n_1,n_2)\)</span></p>
<h5 id="性质-1">性质</h5>
<p><span class="math display">\[ F \sim F(n_1,n_2) \Rightarrow \frac{1}{F} \sim F(n_2, n_1) \]</span> <span class="math display">\[ F_{1-\alpha}(n_1,n_2)=\frac{1}{F_\alpha(n_2,n_1)} \]</span></p>
<h3 id="抽样分布定理">抽样分布定理</h3>
<h4 id="服从抽样分布的随机变量">服从抽样分布的随机变量</h4>
<p>n 个相互独立并服从标准正态分布的随机变量的平方和服从自由度为 n 的 <span class="math inline">\(\chi^2\)</span> 分布：</p>
<p><span class="math display">\[ \sum_{i=1}^n X_i^2 \sim \chi^2(n) \]</span></p>
<p>相互独立的随机变量 X，Y， <span class="math inline">\(X\sim N(0,1),\;Y\sim \chi^2(n)\)</span> 有</p>
<p><span class="math display">\[ T = \frac{X}{\sqrt{Y/n}} \sim t(n) \]</span></p>
<p>相互独立的随机变量 X，Y， <span class="math inline">\(X\sim \chi^2(n_1),\;Y\sim \chi^2(n_2)\)</span> 有</p>
<p><span class="math display">\[ F = \frac{X/n_1}{Y/n_2} \sim F(n_1,n_2) \]</span></p>
<h4 id="单样本抽样分布">单样本抽样分布</h4>
<p>样本 <span class="math inline">\(X_1,\,X_2\,...,X_n\)</span> 来自于分布为 <span class="math inline">\(N(\mu,\sigma^2)\)</span> 的正态总体， <span class="math inline">\(\overline{X},\;S^2\)</span> 分别为样本均值与方差，则有</p>
<p><span class="math display">\[ \overline{X}\; 与 \;S^2\; 相互独立\\
\overline{X} \sim N(\mu,\frac{\sigma^2}{n})\\
\frac{n-1}{\sigma^2}S^2 \sim \chi^2(n-1)\\
\frac{\overline{X}-\mu}{S/\sqrt{n}} \sim t(n-1) \]</span></p>
<h4 id="双样本抽样分布">双样本抽样分布</h4>
<p>两样本分别来自于正态总体 <span class="math inline">\(N(\mu_1,\sigma_1^2)\)</span> 和 <span class="math inline">\(N(\mu_2,\sigma_2^2)\)</span>， 它们相互独立，样本均值和方差分别为 <span class="math inline">\(\overline{X},\;S_1^2\)</span> 及 <span class="math inline">\(\overline{Y},\;S_2^2\)</span>， 样本容量分别为 <span class="math inline">\(n_1,\;n_2\)</span>。 则有</p>
<p><span class="math display">\[\frac{S_1^2/\sigma_1^2}{S_2^2/\sigma_2^2} \sim F(n_1-1,n_2-1)\]</span></p>
<p><span class="math display">\[ with\; \sigma_1^2=\sigma_2^2,\quad
\cfrac{(\overline{X}- \overline{Y})-(\mu_1 - \mu_2)}{S_w\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}} \sim t(n_1+n_2 - 2) \]</span></p>
<p>其中</p>
<p><span class="math display">\[ S_w = \sqrt{\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{n_1+n_2-2}} \]</span></p>
<h4 id="无偏估计">无偏估计</h4>
<p>样本均值与方差的期望分别是总体分布的期望与方差。</p>
<p><span class="math display">\[ E(\overline{X}) = \mu \]</span></p>
<p><span class="math display">\[ E(S^2) = \sigma^2 \]</span></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="//schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/18/Math-LinAlg-quadraticForm/" itemprop="url">
                  二次型
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-09-18T10:41:00+08:00" content="2016-09-18">
              2016-09-18
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Math/" itemprop="url" rel="index">
                    <span itemprop="name">Math</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a href="/2016/05/21/Math-LinAlg-Matrix/" title="矩阵相关">矩阵相关</a>
<h3 id="参考">参考：</h3>
<ul>
<li><a href="http://www.yalewoo.com/linear_algebra_6_quadratic-form.html" target="_blank" rel="noopener">线性代数知识梳理6——二次型</a></li>
<li><a href="/2016/09/17/Math-LinAlg-eigenOrthDiag/" title="矩阵特征值、正交化与对角化">矩阵特征值、正交化与对角化</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E8%A5%BF%E5%B0%94%E7%BB%B4%E6%96%AF%E7%89%B9%E6%83%AF%E6%80%A7%E5%AE%9A%E7%90%86" target="_blank" rel="noopener">西尔维斯特惯性定理</a></li>
</ul>
<h3 id="相关概念">相关概念</h3>
<h4 id="可逆线性变换">可逆线性变换</h4>
<p><span class="math display">\[ \mathbf{X} = \mathbf{CY} \]</span></p>
<p>该公式实现从未知数向量 <span class="math inline">\(\mathbf{Y}\)</span> 到 <span class="math inline">\(\mathbf{X}\)</span> 的线性变换，当 <span class="math inline">\(\mathbf{C}\)</span> 可逆时，称为可逆线性变换。</p>
<h4 id="标准形">标准形</h4>
<p>平方和形式的二次型，其系数矩阵为对角矩阵。</p>
<h4 id="规范形">规范形</h4>
<p>二次型是平方和形式、且系数为 <span class="math inline">\(\pm 1\)</span>， 正在前，负在后。</p>
<h4 id="惯性指数">惯性指数</h4>
<p>标准形中正系数的项数为正惯性指数，负系数的项数为负惯性指数。</p>
<h3 id="二次型化为标准形">二次型化为标准形</h3>
<p>通过可逆线性变换 <span class="math inline">\(\mathbf{X} = \mathbf{CY}\)</span>， 变换二次型 <span class="math inline">\(f(\mathbf{X})=\mathbf{X^T A X}=\mathbf{Y^T (C^T A C) Y}\)</span>。 要求 C 可逆，因为 <strong>可逆线性变换不改变二次型的秩</strong>。<br>
二次型化通过可逆线性变换为标准形，即求可逆 <span class="math inline">\(\mathbf{C}\)</span> 使得系数矩阵 <span class="math inline">\(\mathbf{A}\)</span> 的 <strong>合同</strong> 矩阵 <span class="math inline">\(\mathbf{\Lambda}=\mathbf{C^T A C}\)</span> 为对角矩阵。（当然 <span class="math inline">\(\mathbf{\Lambda}\)</span> 也解出了）整个过程可视作对实对称矩阵 A 的合同对角化。</p>
<h4 id="配方法">配方法</h4>
<p>对 x 挨个配方，再用 x 表示 y 构成方程组，求该方程组系数矩阵的逆矩阵即 <span class="math inline">\(\mathbf{C}\)</span>， 标准形在配方时可知。 （参考：<a href="http://www.yalewoo.com/linear_algebra_6_quadratic-form.html" target="_blank" rel="noopener">线性代数知识梳理6——二次型</a>）</p>
<h4 id="正交变换">正交变换</h4>
<p>在可逆线性变换 <span class="math inline">\(\mathbf{X} = \mathbf{CY}\)</span> 的基础上，进一步要求 <span class="math inline">\(\mathbf{C}\)</span> 为正交矩阵。<br>
求解步骤参考 <a href="/2016/09/17/Math-LinAlg-eigenOrthDiag/" title="实对称矩阵的相似对角化">实对称矩阵的相似对角化</a>。</p>
<h3 id="相关定理">相关定理</h3>
<h4 id="惯性定理">惯性定理</h4>
<p>二次型通过可逆线性变换能化为规范形且规范形唯一。所以， <em>可逆线性变换不改变二次型的秩和正负惯性指数</em>。（参考：<a href="https://zh.wikipedia.org/wiki/%E8%A5%BF%E5%B0%94%E7%BB%B4%E6%96%AF%E7%89%B9%E6%83%AF%E6%80%A7%E5%AE%9A%E7%90%86" target="_blank" rel="noopener">西尔维斯特惯性定理</a>）</p>
<h4 id="正定矩阵的充要条件">正定矩阵的充要条件</h4>
<p>对实对称矩阵 <span class="math inline">\(\mathbf{A}\)</span></p>
<ul>
<li>特征值全为正实数</li>
<li>与单位矩阵合同</li>
<li>顺序主子式全大于零</li>
</ul>
<p><em>顺序主子式</em>： 方阵的前 k 行 k 列的行列式</p>
<h4 id="负定矩阵的充要条件">负定矩阵的充要条件</h4>
<p>对实对称矩阵 <span class="math inline">\(\mathbf{A}\)</span></p>
<ul>
<li>特征值全为负实数</li>
<li>二次型负惯性指数为 n</li>
<li>顺序主子式满足： <span class="math inline">\((-1)^k P_k &gt;0\;\;(k=1,2,...,n)\)</span></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="//schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/17/Math-LinAlg-eigenOrthDiag/" itemprop="url">
                  矩阵特征值、正交化与对角化
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-09-17T15:12:13+08:00" content="2016-09-17">
              2016-09-17
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Math/" itemprop="url" rel="index">
                    <span itemprop="name">Math</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a href="/2016/05/21/Math-LinAlg-Matrix/" title="矩阵相关">矩阵相关</a>
<p>几何上看，特征向量乘对应矩阵相当于做一次“伸缩”变换。 <strong>每个方阵都有（实数或复数）特征值</strong>， 而一个特征值对应无数个特征向量。<br>
对角化指 <strong>相似对角化</strong>， 指求解与方阵 <strong>相似</strong> 的对角矩阵的过程。<br>
正交化指求解与线性无关向量组 <strong>等价</strong> 的正交向量组的过程，因为等价，两个向量组在同一子空间。</p>
<h3 id="相关概念">相关概念</h3>
<h4 id="特征子空间eigenspace">特征子空间（eigenspace）</h4>
<p>具有相同特征值的特征向量与一个同维数的零向量的集合。</p>
<h4 id="特征方程">特征方程</h4>
<p>关于 <span class="math inline">\(\lambda\)</span> 的方程</p>
<p><span class="math display">\[ \det (\lambda \mathbf{I}-\mathbf{A}) = 0 \]</span></p>
<p>称为方阵 <span class="math inline">\(\mathbf{A}\)</span> 的特征方程。由特征值的定义公式推导，其解为方阵 <span class="math inline">\(\mathbf{A}\)</span> 的特征值，被称为 <strong>特征根</strong>， 根据解的重数分为单特征根与 k 重特征根。</p>
<h4 id="特征值的代数重数与几何重数">特征值的代数重数与几何重数</h4>
<p>特征方程特征值（特征根） <span class="math inline">\(\lambda\)</span> 的重数称为 <span class="math inline">\(\lambda\)</span> 的代数重数，而 <span class="math inline">\(\lambda\)</span> 的特征子空间的维数（基向量个数）称为几何重数。 <strong>特征值的几何重数不大于它的代数重数</strong>。</p>
<h3 id="特征值与特征向量的计算">特征值与特征向量的计算</h3>
<ol type="1">
<li>根据特征方程 <span class="math inline">\(\det (\lambda \mathbf{I}-\mathbf{A}) = 0\)</span> 计算全部特征根 <span class="math inline">\(\lambda_1 \sim \lambda_k\;(k \le n)\)</span></li>
<li>线性方程组 <span class="math inline">\((\lambda_i \mathbf{I}-\mathbf{A})\mathbf{X}=\mathbf{0}\)</span> 的通解 <strong>除去零向量</strong> 即对应于 <span class="math inline">\(\lambda_i\)</span> 的所有特征向量</li>
</ol>
<h3 id="一般矩阵的相似对角化">一般矩阵的相似对角化</h3>
<p><span class="math display">\[ \mathbf{P}^{-1}\mathbf{A}\mathbf{P} = \mathbf{Diag}[\lambda_1, \lambda_2,...,\lambda_n] \]</span></p>
<p>现需求解 <span class="math inline">\(\lambda\)</span> 与 <span class="math inline">\(\mathbf{P}\)</span>。首先要求 A 能对角化。</p>
<ol type="1">
<li>求解 A 的特征值 <span class="math inline">\(\lambda\)</span>， 相似对角矩阵的对角元素即 <span class="math inline">\(\lambda\)</span>， 每个特征值出现的次数与其代数重数相等</li>
<li>求解每个特征值对应的特征向量，检查是否满足对角化条件。P 即特征列向量构成的矩阵，排列顺序与特征值相同。</li>
</ol>
<h3 id="施密特正交化">施密特正交化</h3>
<p>前提是 <strong>向量组线性无关</strong>。 对 <span class="math inline">\(\alpha_1,\alpha_2,...,\alpha_s\)</span> 施密特正交化：</p>
<p><span class="math inline">\(\beta_1 = \alpha_1\)</span><br>
<span class="math inline">\(\beta_2 = \alpha_2 - (\alpha_2\cdot \beta_1)/(\beta_1\cdot \beta_1)\,\beta_1\)</span><br>
<span class="math inline">\(\beta_3 = \alpha_3 - (\alpha_3\cdot \beta_1)/(\beta_1\cdot \beta_1)\,\beta_1 - (\alpha_3\cdot \beta_2)/(\beta_2\cdot \beta_2)\,\beta_2\)</span><br>
…<br>
<span class="math inline">\(\beta_s = \alpha_s - (\alpha_s\cdot \beta_1)/(\beta_1\cdot \beta_1)\,\beta_1 - (\alpha_s\cdot \beta_2)/(\beta_2\cdot \beta_2)\,\beta_2-... - (\alpha_s\cdot \beta_{s-1})/(\beta_{s-1}\cdot \beta_{s-1})\,\beta_{s-1}\)</span></p>
<p>得到正交向量组，再令 <span class="math inline">\(\gamma_i = \beta_i / (|\beta_i|)\)</span> 得到标准正交向量组</p>
<h3 id="实对称矩阵的相似对角化">实对称矩阵的相似对角化</h3>
<p>与一般矩阵的相似对角化相同，但习惯更进一步，要求 <span class="math inline">\(\mathbf{P}\)</span> 正交，所以在对每个特征值求解出特征向量组后，需要做标准正交化处理。（不同特征值的特征向量彼此正交，不必处理）</p>
<h3 id="性质">性质</h3>
<h4 id="特征值的和积">特征值的和积</h4>
<p><span class="math display">\[ \lambda_1 + \lambda_2 + ... \lambda_n = a_{1,1} + a_{2,2} + ... + a_{n,n} \\
\lambda_1\lambda_2...\lambda_n = \det \mathbf{A} \]</span></p>
<p>其中包括复数特征根。可得 <strong>方阵可逆的充要条件是其所有特征值全不为零</strong>。</p>
<h4 id="不同特征值的特征向量线性无关">不同特征值的特征向量线性无关</h4>
<h4 id="对角化与特征值的相关定理">对角化与特征值的相关定理</h4>
<ul>
<li><strong>相似矩阵的特征值相同</strong></li>
<li>n 阶方阵能对角化的充要条件
<ul>
<li>A 的每个特征值 <span class="math inline">\(\lambda_i\)</span> 的代数重数 <span class="math inline">\(k_i\)</span> 等于方程组 <span class="math inline">\((\lambda_i \mathbf{I} - \mathbf{A}) \mathbf{X} = \mathbf{0}\)</span> 的基础解系向量个数，即 <span class="math inline">\(k_i=n - R(\lambda_i \mathbf{I} - \mathbf{A})\)</span></li>
<li>A 有 n 个线性无关的特征向量</li>
<li>充分条件：特征值都是单特征根</li>
</ul></li>
<li>相似对角方阵的对角元素是全部特征值</li>
</ul>
<h4 id="正交矩阵的相关性质">正交矩阵的相关性质</h4>
<ul>
<li><span class="math inline">\(\det \mathbf{A} = \pm 1\)</span></li>
<li><span class="math inline">\(\mathbf{A},\;\mathbf{B}\)</span> 正交，则 <span class="math inline">\(\mathbf{AB}\)</span> 正交</li>
</ul>
<h4 id="实对称矩阵相关性质">实对称矩阵相关性质</h4>
<ul>
<li>特征值都是实数</li>
<li>不同特征值的特征向量彼此正交</li>
<li>一定有 <strong>正交方阵</strong> <span class="math inline">\(\mathbf{C}\)</span> 使之相似对角化 <span class="math inline">\(\mathbf{C^{-1}AC}=\mathbf{C^TAC}\)</span></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="//schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Blunt" />
          <p class="site-author-name" itemprop="name">Blunt</p>
          <p class="site-description motion-element" itemprop="description">email：summer15y@163.com</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">62</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">29</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Blunt</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.2"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.2"></script>



  



  




  
  

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


  

  

  


</body>
</html>
